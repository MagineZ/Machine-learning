---
title: "Practical Machine Learning Peer Assessments"
author: "Pei-Chun Su"
date: "Saturday, February 13, 2016"
output: html_document
---

##Executive Summary
This document presents the results of the Practical Machine Learning Peer Assessments in a report using a single R markdown document that can be processed by knitr and be transformed into an HTML file.  

##Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.  

In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

##Data
The training data for this project are downloaded from: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  

The test data are downloaded from: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

It is worth citing Groupware@LES for being generous in allowing their data to be used for this assignment.

##Goal of the assignment
Predicting the manner in which the participants did the exercise. Refer to the ¡§classe¡¨ variable in the training set. All other variables can be used as predictor.

Show how the model was built, performed cross validation, and expectation of the sample error and reasons of choices made.

Use the prediction model to predict 20 different test cases.

##Data Preprocessing
```{r,cache=TRUE}
library(caret)
library(rpart)
library(knitr)
library(randomForest)
library(ElemStatLearn)
library(corrplot)
set.seed(666) # For research reproducibility purpose
setwd("C:/Users/Pei-Chun/Documents/R/Machine learning")
```
##Preparation of Datasets
```{r,cache=TRUE}
trainRaw <- read.csv("./pml-training.csv",header=T,sep=",",na.strings=c("NA",""))
testRaw <- read.csv("./pml-testing.csv",header=T,sep=",",na.strings=c("NA",""))
```
  
##Data Sets Partitioning Definitions
The data partitions of training and validating data sets are created as below:  
```{r,cache=TRUE}
trainRaw <- trainRaw[,-1] # Remove the first column that represents a ID Row
inTrain = createDataPartition(trainRaw$classe, p=0.60, list=F)
training = trainRaw[inTrain,]
validating = trainRaw[-inTrain,]
```
##Data Cleaning
Since a random forest model is chosen and the data set must first be checked on possibility of columns without data.  
  
The decision is made whereby all the columns that having less than 60% of data filled are removed.  
```{r,cache=TRUE}  
sum((colSums(!is.na(training[,-ncol(training)])) < 0.6*nrow(training)))
```
Next, the criteria to remove columns that do not satisfy is applied before applying to the model.  
```{r,cache=TRUE}
Keep <- c((colSums(!is.na(training[,-ncol(training)])) >= 0.6*nrow(training)))
training   <-  training[,Keep]
validating <- validating[,Keep]
```
##Modeling
In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the execution. Therefore, the training of the model (Random Forest) is proceeded using the training data set.
```{r,cache=TRUE}
model <- randomForest(classe~.,data=training)
model
```
##Model Evaluation
Verification of the variable importance measures as produced by random Forest is as follows:
```{r,cache=TRUE}
importance(model)
```
Next, the model results is evaluated through the confusion Matrix.  
```{r,cache=TRUE}
confusionMatrix(predict(model,newdata=validating[,-ncol(validating)]),validating$classe)
```
The accurancy for the validating data set is calculated with the following formula:
```{r,cache=TRUE}
acrcy<-c(as.numeric(predict(model,newdata=validating[,-ncol(validating)])==validating$classe))
acrcy<-sum(acrcy)*100/nrow(validating)
```
Model Accuracy as tested over Validation set = 99.73% The out-of-sample error is only 0.17%.
  
##Model Test
For the model testing, the new values are predicted using the testing dataset provided which was loaded earlier. Data cleaning was first performed and all columns of Testing data set are coerced for the same class of previous data set.  
```{r,cache=TRUE}
testRaw <- testRaw[,-1] # Remove the first column that represents a ID Row
testRaw <- testRaw[ , Keep] # Keep the same columns of testing dataset
testRaw <- testRaw[,-ncol(testRaw)] # Remove the problem ID
```
Transformations and Coercing of Testing Dataset  
```{r,cache=TRUE}
# Coerce testing dataset to same class and structure of training dataset 
testing <- rbind(training[100, -59] , testRaw) 

# Apply the ID Row to row.names and 100 for dummy row from testing dataset 
row.names(testing) <- c(100, 1:20)
```
##Prediction with the Testing Dataset
```{r,cache=TRUE}
predictions <- predict(model,newdata=testing[-1,])
predictions
```
We submitted these prediction ot the quzz and all are correct. 